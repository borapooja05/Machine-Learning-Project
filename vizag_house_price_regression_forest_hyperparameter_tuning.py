# -*- coding: utf-8 -*-
"""Vizag House Price  Regression Forest hyperparameter tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lsa-YaxCAVuFtFw8VWR5ZofPkpPFVKmg
"""

import pandas as pd
import seaborn as sns
import matplotlib as plt

df=pd.read_csv('/content/india_housing_prices.csv')
vizag_df=df[(df['State']=='Andhra Pradesh') & (df['City']=='Vishakhapatnam')]
vizag_df.drop(['State','City','ID'], axis=1,inplace=True)
vizag_df.info()

vizag_df.nunique()

"""Step1) Convert all columns to numerical - One Hot Encoding, Label encoding (order makes sense)


Step2) convert them to datatype category - df.<column name>.astype("category"). Need our tree to know that it needs to treat these numerical values as categories

Step3)check for missing values if any, ? etc


Step4) sns.pairplot(df) - all pair of variables plotted
Check if any outliers present


Step5)boxplot for each col with price_in_lakhs - outliers


Step6) create a list of top outliers-their indices. of price_in-lakhs

drop or cap these outliers - I think this step could be skipped as random forest is spposed to be insensitive to outliers?


For re-usability : create these below steps as functions

step7) Start with all variables for now.
Split X_train, y_train, X_test, y_test


step8)build preliminary regression forest, with some parameters set randomly

n_estimators: number of trees in forest

max_features: at a time how many features will be picked to decide root/branch

max_samples: while bootstrapping how may records to be kept in one bag

oob_score: out of bag score. Sum of squares residuals found for out of bag records (testing done on these to understand good fit of model)

by default repitition is enabled fo bootstrapping

model=RandomForestRegressor(n_estimators=500, max_features=3, max_samples=60, oob_score=True)

rf=model.fit(X_train,y_train)

step9) rf.oob_score_
to know goodness of fit for the model

Step10) Inspect which variables were important
rf.feature_importances_
numpy array for scores (sum up to 1)

rf.feature_names_
Lists names of features

Step11) barplot these feature importance vs feature names

Step12) y_pred=rf.predict(X_test)

Evaluate performance by using base MAE, MSE, RMSE

step13) Reiterate - based on variable importance plot, remove variables which aren;t important. Feature enginnering - add new variables

Re-check model performance

Note: Decision forests are not sensitive to scale

Step14)scatter plot prediction vs true value

TP,FP,TN,FN matrix creation

Step15) Understand why some points have large deviation - decide what to do with these? delete from dataset? or something else?

Step16) how to improve performance by modifying parameters? Should I be doind cross validation? - Explore more on this

Step17) create pickle file for this model so that it can be used for any price prediction data.

with open("<nameOfFile>.pkl", wb) as f:
  pickle.dump(rf,f)
"""

vizag_df.info()

pip install category_encoders

one_hot_encoding_list=['Owner_Type', 'Facing', 'Furnished_Status', 'Property_Type']
binary_encoding_list=['Parking_Space', 'Security', 'Availability_Status']
label_encoding_list=['Public_Transport_Accessibility']
frequency_encoding=['Location']

import category_encoders as ce

#Label encoding
vizag_df['Public_Transport_Accessibility']=vizag_df['Public_Transport_Accessibility'].replace({'Low':1,'Medium':2,'High':3})

frequency_encod_map=vizag_df['Locality'].value_counts().to_dict()
vizag_df['Locality_freq']=vizag_df['Locality'].replace(frequency_encod_map)
vizag_df.drop(['Locality'],axis=1,inplace=True)

#vizag_encoded=pd.get_dummies(vizag_df, columns=one_hot_encoding_list,dummy_na=True,dtype=int)
#vizag_encoded.nunique()
#verified that there were no nan values in dataset for categories so removing dummy_na
vizag_OneHotEncoded=pd.get_dummies(vizag_df, columns=one_hot_encoding_list,dtype=int)

#Initialize the BinaryEncoder
encoder = ce.BinaryEncoder(cols=binary_encoding_list)

# Apply binary encoding
vizag_encoded = encoder.fit_transform(vizag_OneHotEncoded)
#or use replace function directly for binary, label encoding

vizag_encoded.dtypes

#amenities to be created in One-Hot encoding format, no need of amenity score like it was done in linear regression

amenities = ['Pool', 'Gym', 'Clubhouse', 'Garden', 'Playground']

# Create new columns for each amenity and initialize with 0
for amenity in amenities:
    vizag_encoded[amenity] = 1
vizag_encoded.drop(['Amenities'],axis=1,inplace=True)

#convert data type to category

numerical_cols=['Size_in_SqFt', 'Price_in_Lakhs', 'Price_per_SqFt', 'Year_Built',
       'Floor_No', 'Total_Floors', 'Age_of_Property', 'Nearby_Schools',
       'Nearby_Hospitals', 'Locality_freq']

categorical_cols=['BHK', 'Public_Transport_Accessibility', 'Parking_Space_0',
       'Parking_Space_1', 'Security_0', 'Security_1', 'Availability_Status_0',
       'Availability_Status_1', 'Owner_Type_Broker',
       'Owner_Type_Builder', 'Owner_Type_Owner', 'Facing_East', 'Facing_North',
       'Facing_South', 'Facing_West', 'Furnished_Status_Furnished',
       'Furnished_Status_Semi-furnished', 'Furnished_Status_Unfurnished',
       'Property_Type_Apartment', 'Property_Type_Independent House',
       'Property_Type_Villa', 'Pool', 'Gym', 'Clubhouse', 'Garden',
       'Playground']
vizag_encoded[categorical_cols]=vizag_encoded[categorical_cols].astype('category')
vizag_encoded.info()

#check data
sns.pairplot(vizag_encoded)

"""Price per sq ft has some outliers in extreme end (very expensive)
some homes are small in size but have high price per sqft'
"""

#boxplot for each column with Price_in_Lakhs
import matplotlib.pyplot as plt

# Determine the number of rows and columns for the subplots
n_features = len(vizag_encoded.columns)
n_cols = 2  # You can adjust this for desired layout
n_rows = (n_features + n_cols - 1) // n_cols # Calculate rows needed

# Create a figure and a set of subplots
fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 5 * n_rows))

# Flatten the axes array if there's more than one row/column
if n_rows > 1 or n_cols > 1:
    axes = axes.flatten()

# Loop through the features and plot histograms
for i, feature in enumerate(vizag_encoded.columns):
    if i < len(axes): # Ensure we don't try to plot more than available subplots
        sns.boxplot(x=vizag_encoded[feature],y=vizag_encoded["Price_in_Lakhs"] ,ax=axes[i])

plt.tight_layout() # Adjust layout to prevent overlapping titles/labels
plt.show()

sns.boxplot(vizag_encoded["Price_per_SqFt"])

sns.boxplot(vizag_encoded["Size_in_SqFt"])

"""Price per sqft has lot of outliers (higher end) - what should I do with this?

Against Price_in_Lakhs - apart from price_per_sqft, no outliers found

I'm going to keep these outliers and go ahead with random forest as ideally random forest should be able to deal with outliers
"""

#split data in train and test-let's build a baseline random forest first.
#later I'll check if it could be optimised

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

def split_Data_train_model(vizag_encoded,n_tree,f,s):
  X=vizag_encoded.drop(['Price_in_Lakhs'], axis=1,inplace=False)
  y=vizag_encoded['Price_in_Lakhs']
  X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.2)
  model=RandomForestRegressor(n_estimators=n_tree, max_features=f, max_samples=s, oob_score=True)
  rf=model.fit(X_train,y_train)
  return rf,X_test,y_test,X_train,y_train

#baseline random forest
model=RandomForestRegressor(n_estimators=500, max_features=3, max_samples=60, oob_score=True)
rf=model.fit(X_train,y_train)

rf.oob_score_

rf.ccp_alpha

"""Forget about setting alpha in random forest. It's difficult to set alpha for each decision tree in random forest

Rather focus on other parameters like number of trees, max depth
"""

print("feature importance",rf.feature_importances_) #numpy array
print("feature names",rf.feature_names_in_) #numpy array

"""Feature importance is displayed in the same order of columns as given in train dataset"""

def feature_importance_rf(rf):
  feature_name_list=list(rf.feature_names_in_)
  feature_importance_list=list(rf.feature_importances_)
  feature_name_importance_df=pd.DataFrame({"feature name": feature_name_list, "feature importance": feature_importance_list})
  print(feature_name_importance_df.sort_values(by="feature importance", ascending=False))
feature_importance_rf(rf)

#sns.barplot(data=feature_name_importance_df, x="feature importance", y="feature name", sort=descending)

sns.barplot(data=feature_name_importance_df, x="feature importance", y="feature name", order=feature_name_importance_df.sort_values(by="feature importance", ascending=False)["feature name"])

"""OOB score still gets generated on training dataset (unused in bootstrapping)

It's a good idea to test on completely unseen data too

I'll use MAE, MSE, RMSE, R square
"""

from math import sqrt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def evaluate_model_unseen_test_Data(rf,X_test,y_test):
  y_pred=rf.predict(X_test)
  print("MAE",mean_absolute_error(y_test,y_pred))
  print("MSE",mean_squared_error(y_test,y_pred))
  print("RMSE", sqrt(mean_squared_error(y_test,y_pred)))
  print("R square", r2_score(y_test,y_pred))

#this is the baseline forest
rf,X_test,y_test=split_Data_train_model(vizag_encoded,500,3,60)
evaluate_model_unseen_test_Data(rf,X_test,y_test)

"""Two things to do:
1) Will feature selection help in improving model?

2)Can max depth, number of trees, max features parameters be chosen for an optimized model?

Also want to check why are we getting such high error. It is because of some outliers?
"""

#scatter point for predicted and actual values
y_pred=rf.predict(X_test)
sns.scatterplot(x=y_pred,y=y_test)
plt.xlabel("predicted")
plt.ylabel("actual")

"""High end actual prices were not captured/predicted clearly by random forest- off by half the amount. If actual price is 500 lakhs, we're predicting 260 lakhs

For some cases actual price was around 100 lakhs but we predicted more than 250 lakhs

I'll check if feature enginnering helps first before I select feature
"""

vizag_encoded_feature_engineered=vizag_encoded.copy()
vizag_encoded_feature_engineered["Hospital_school_nearby"]=vizag_encoded_feature_engineered["Nearby_Hospitals"]+vizag_encoded_feature_engineered["Nearby_Schools"]
vizag_encoded_feature_engineered["Floor_wrt_building"]=vizag_encoded_feature_engineered["Floor_No"]/vizag_encoded_feature_engineered["Total_Floors"]
vizag_encoded_feature_engineered["Price_total_area"]=vizag_encoded_feature_engineered["Price_per_SqFt"]*vizag_encoded_feature_engineered["Size_in_SqFt"]
vizag_encoded_feature_engineered.drop(["Nearby_Hospitals","Nearby_Schools","Floor_No","Total_Floors"],axis=1,inplace=True)

#building and evaluating random forest with feature engineering
rf2,X2_test,y2_test,X2_train,y2_train=split_Data_train_model(vizag_encoded_feature_engineered,500,3,60)
evaluate_model_unseen_test_Data(rf2,X2_test,y2_test)

"""Any further improvement could happen if we select features using feature importance?"""

feature_importance_rf(rf2)

#drop features with importance less than 0.01
vizag_encoded_feature_engineered_selection=vizag_encoded_feature_engineered.copy()
vizag_encoded_feature_engineered_selection.drop(["Facing_East","Facing_West","Pool","Gym","Clubhouse","Garden","Playground"],axis=1,inplace=True)
rf3,X3_test,y3_test,X3_train,y3_train=split_Data_train_model(vizag_encoded_feature_engineered_selection,500,3,60)
evaluate_model_unseen_test_Data(rf3,X3_test,y3_test)

"""Improvement baby!!

Now let's see if we can optimise further by changing n_tree, max_depth, max features

Methods:

Grid Search

Randomised search

Bayesian optimization
"""

#Grid search
n_trees=[20,60,150,500]
max_depth=[10,50,100]
max_features=[3,7,10]
from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': n_trees,
    'max_depth': max_depth,
    'max_features': max_features
}

rf2_grid=GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)
#verbose=2 will show me what's going on behidn the scene
#n_job=-1 will ensure all resources allocated

rf2_grid.fit(X3_train,y3_train)

print("best values of parameters as per given list",rf2_grid.best_params_)
print("best score",rf2_grid.best_score_)

"""Seems like it just picked highest value of each parameter! Perhaps reduce if it takes too much time to run on huge datasets, I could decrease the highest value of parameter and check performance of model if there's significant difference

Check performance by test dataset using these parameters
"""

model1=RandomForestRegressor(n_estimators=500, max_features=10,max_depth=50,oob_score=True)
best_param_rf=model1.fit(X3_train,y3_train)
evaluate_model_unseen_test_Data(best_param_rf,X3_test,y3_test)

"""Repeat testing using cross validation and find average score of this model with these parameters"""

from sklearn.model_selection import cross_val_score
cross_val_score(best_param_rf,X3_train,y3_train,cv=5)

"""All 5 folds almost gave similar performance so I'm satisfied. If one of the fold had given lesser score, then I would have been worried. BTW what's the default score mechanism that cross val score picks? what's this score we're referring to? - think it's R square

Let's repeat the same using Randomised search for hyperparameter tuning
"""

n_trees=[20,60,150,500,1000]
max_depth=[10,50,100,None]
max_features=[3,7,10]
sample_percentage=[0.3,0.5,0.75]
min_items_inLeaf=[1,2,5]
bootstrap=[True,False]
from sklearn.model_selection import RandomizedSearchCV
param_grid = {
    'n_estimators': n_trees,
    'max_depth': max_depth,
    'max_features': max_features,
    'min_samples_leaf':min_items_inLeaf,
   # 'bootstrap':bootstrap,
    'max_samples':sample_percentage
}

rf2_random=RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_grid, cv=5, verbose=2, n_jobs=-1)
rf2_random.fit(X3_train,y3_train)
print("best values of parameters as per given list using Random search CV",rf2_random.best_params_)
print("best score using Random search CV",rf2_random.best_score_)

model3=RandomForestRegressor(n_estimators=60, max_samples=0.5 , max_features= 10, min_samples_leaf=1, oob_score=True)
best_param_rf_randomS=model3.fit(X3_train,y3_train)
evaluate_model_unseen_test_Data(best_param_rf_randomS,X3_test,y3_test)

cross_val_score(best_param_rf_randomS, X3_train, y3_train, cv=5)

"""Touch base on last optimized method -Naive Bayesian"""