# -*- coding: utf-8 -*-
"""Decision Tree implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pYi54I7m9X456T7n4FW4w0Si_HUTP0Qv
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

cl_df=pd.read_csv('/content/processed.cleveland.data.csv', header=None)
cl_df.head()

"""Got column names from website:

age
sex
cp
trestbps
chol
fbs
restecg
thalach
exang
oldpeak


Setup these as headers
"""

cl_df.columns=[
'age',
'sex',
'cp',
'trestbps',
'chol',
'fbs',
'restecg',
'thalach',
'exang',
'oldpeak','slope'
,'ca',
'thal',
'hd'	]

cl_df.head()

"""Identify missing data"""

cl_df.isnull().sum()
#no missing data present here. This is preprocessed file

cl_df.dtypes

"""ca, thal why are they objects?"""

print("unique values in ca col",cl_df.ca.unique())
print("unique values in thal col",cl_df.thal.unique())

"""isnull().sum() would only help if we have null values

Here ? is the missing data.

How many records have ?
"""

print("count of unique values in ca col",cl_df.ca.value_counts())
print("count of unique values in thal col",cl_df.thal.value_counts())

cl_df.shape

"""So tota 6 records have missing data out of 303 i.e. ~2% of data

For purpose of learning Random forest, simply remove the missing records
"""

#view records without missing values
cl_nomissing_df=cl_df.loc[(cl_df.ca!='?')& (cl_df.thal !='?')]
cl_nomissing_df.ca.value_counts()

"""Format data for classification tree

Split the data in independent variables X, target y

Then format data in X
"""

X=cl_nomissing_df.drop('hd', axis=1, inplace=False).copy()
y=cl_nomissing_df.hd.copy()

"""scikit learn needs data to be in numerical format. Convert categorical to features-One Hot Encoding.
Because the number assigned to category doesn't have an order or meaning behind the number. Like category 3,4 are not necessarily closely related. We've assigned random number to the category. Decision tree would however treat 4 to be related closely to 3 compared to number 1.


X has categorical data stored as float. Change this and then apply One Hot Encoding
"""

X.nunique()

"""do I keep , drop_first=True? what if it'll remove category which plays crucial role in decision tree?

Decision Tree will take care of this. If there's redundancy like with One Hot ecnoding without dropping redundant columns - Decision tree will randomly pick the first column which is significant to making decision.

For decision tree, drop step can be skipped while performing One Hot Encoding
"""

#if I don't specify dtype=int, it will give True/False instead of 1/0
X_encoded=pd.get_dummies(X, columns=['cp','sex','fbs','slope','ca','restecg','exang','thal'], dtype=int)
#
X_encoded

#converting y into binary class - we can build multi class target variable decision tree but here
#we will stick to binary

#print(y.value_counts())
#since sum of count for other than 0 values is similar to count of 0, I'm going to club >0 under 1 category

y_df=pd.DataFrame(y)
y_df.loc[y_df.hd>0]=1
y_df.nunique()

"""Start with classification. Formatting data is completed"""

#split into train, test
X_train, X_test, y_train, y_test = train_test_split(X_encoded,y_df,  random_state=42)
X_train.shape #222 records in train

from sklearn.tree import DecisionTreeClassifier
clf_tree=DecisionTreeClassifier(random_state=42)
clf_tree=clf_tree.fit(X_train, y_train)
clf_tree

"""Since we have smaller dataset, limited number of features - DecisionTree works fine
If there were too many categories, features too many - use other models like XGBoost
"""

#print decision tree created
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

fig=plt.figure(figsize=(15,7.5))
plot_tree(clf_tree, filled=True,rounded=True,class_names=["No HD","Yes HD"], feature_names=X_train.columns)

"""We've created a preliminary decision tree (baseline) which is not optimized right now.

We'll cover how to optimize this in a bit

Check how it performs against test dataset now.
Confusion matrix created- Predicted value vs Actual value
"""

import sklearn.metrics as mt
from sklearn.metrics import ConfusionMatrixDisplay

print(mt.confusion_matrix(y_test, clf_tree.predict(X_test)))
ConfusionMatrixDisplay.from_estimator(clf_tree, X_test, y_test,display_labels=["No HD","Yes HD"])

"""17 test records are being incorrectly classified out of 75

Accuracy=58/75 = 0.773

TPR = 27/33 = 0.81

FPR = 11/37 = 0.297

Precision=27/38 = 0.71
"""

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report
print("Accuracy score", accuracy_score(y_test, clf_tree.predict(X_test)))
print("recall score", recall_score(y_test, clf_tree.predict(X_test)))
print("precision score", precision_score(y_test, clf_tree.predict(X_test)))

"""Let's see if this tree is overfitted and if it can be optimized further.

Cost complexity pruning will be covered here

First get all possible list of alphas using which pruned trees can be built except for the last one where only root node is defined (not a tree) - We will do this on train dataset only, not on entire dataset or test dataset because we're trying to address overfitting issue here
"""

clf_tree.cost_complexity_pruning_path(X_train,y_train) #to determine values of alpha

path=clf_tree.cost_complexity_pruning_path(X_train,y_train)
ccp_alphas=path.ccp_alphas
ccp_alphas=ccp_alphas[:-1] #exclude last alpha which will give just the root node without a tree
ccp_alphas

#now create a list of pruned decision trees created by using each alpha based on training data only
clfs=[]
for ccp_alpha in ccp_alphas:
  clf_dt=DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
  clf_dt.fit(X_train, y_train)
  clfs.append(clf_dt)
clfs

"""Then we'll check accuracy for train and test data wth these alpha values.

Plot them together
"""

#fetch accuracy score for each pruned decision tree generated by all these values of alpha on train and test data
train_score=[clf.score(X_train, y_train) for clf in clfs]
test_score=[clf.score(X_test, y_test) for clf in clfs]

print(test_score)

#plot alpha value vs accuracy score for both train and test datasets
fig, ax=plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_score, marker='o', label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_score, marker='o', label="test", drawstyle="steps-post")

"""As seen, with alpha=0 i.e. our preliminary tree that we built, train data has great accuracy but not for testing data.

Seems like around alpha=0.16 , both training and test data have good accuracy

Since we're using decision tree for classification, default method to build tree is by using Gini impurity. Gini is always between 0-1. Hence alpha values range is quite small.

If regression tree was built, alpha value could be large as we might be using sum of squared residuals to build the tree

Till now, we had divided data in one way and we're assuming that alpha value of 0.016 could be the optimum value.

Let's check further by subdividing data in multiple ways - get new train, test set and check if this alpha value is still optimum value for most cases i.e. k fold cross validation performed
"""

#checking performance of alpha=0.16 with multiple combinations on train data
from sklearn.model_selection import cross_val_score
clf_tree1=DecisionTreeClassifier(random_state=42, ccp_alpha=0.016)

#cross_val_score_entire_dataset=cross_val_score(clf_tree, X_encoded, y_df, cv=20)
#print(cross_val_score_entire_dataset)

cross_val_scores_train_dataset=cross_val_score(clf_tree1, X_train, y_train, cv=10)
print(cross_val_scores_train_dataset)

"""
with cv=5 folds, we see one decision tree has high score and one with very low score

Experiment with other values of k fold-still difference is observed"""

#create dataframe to store these score of cross validation against each k fold
cross_val_df=pd.DataFrame(cross_val_scores_train_dataset, columns=['score'])
cross_val_df.plot()
#plot the score for different subsets of train data

df=pd.DataFrame(data={"trees":range(5),"accuracy":cross_val_scores_train_dataset})
df.plot(x="trees",y="accuracy",linestyle='--')

"""Seems like alpha is sensitive to subset of data being used- model's accuracy varies in some cases

We'll use cross validation to find optimum value of alpha. i.e. for different subsets of data, find which alpha value is optimum (won't keep alpha fixed here obv)

We already have the list of alphas to generate pruned decision trees based on 70% of our entire data (intial train data)
We need to check for which other alpha values, if we apply cross validation - for most subset combo of data do we get maximum accuracy score.
"""

#store list of alpha and the corresponding average accuracy score
#generated by this alpha with various k fold subsets of data
alpha_scores=[]
for alpha in ccp_alphas:
  clf_tree2=DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)
  cv_scores=cross_val_score(clf_tree2, X_train, y_train, cv=5)
  #print(cv_scores)
  #print("alpha",alpha)

  #we want t take average of the scores generated by different folds for each alpha

  #print("average score",np.mean(cv_scores))
  alpha_scores.append((alpha,np.mean(cv_scores),np.std(cv_scores)))
alpha_scores

#plot alpha values against mean score generated using various subsets
alpha_Df=pd.DataFrame(alpha_scores, columns=["alpha","mean_score","std_score"])
alpha_Df.plot()

"""The above graph just takes default index as x axis and charts all numerical cols against y axis. Is difficult to interpret. Not even sure if it makes any sense. Plotting against default index doesn't make any sense/ add value

In below graph, we've added error factor - lines indicate range of error given by standard deviation value here. x axis is alpha here not just a default index.
"""

alpha_Df.plot(x="alpha",y="mean_score",yerr="std_score", marker='o')

"""Seems like around 0.014 might give better results than 0.016 that we got earlier.

Zoom in on this range of 0.014-0.015 and check exact alpha value at which max average score achieved
"""

#zoom in on 0.014 -0.015 alpha ranges and get their mean score
alpha_scores #list of list -alpha, mean score, std dev
alpha_result=pd.DataFrame(alpha_scores,columns=["alpha","mean_score","std_dev"])
ideal_ccp_alpha_df=alpha_result.loc[(alpha_result["alpha"] >= 0.014) & (alpha_result["alpha"] < 0.015)]
ideal_ccp_alpha=float(ideal_ccp_alpha_df["alpha"] ) #returns series but we need a float value
type(ideal_ccp_alpha)

"""Now pass this optimum value of alpha to build decision tree -our final optimized tree using
ideal_ccp_alpha
"""

final_clf_tree=DecisionTreeClassifier(random_state=42, ccp_alpha=ideal_ccp_alpha)
final_clf_tree.fit(X_train, y_train)


fig=plt.figure(figsize=(15,7.5))
plot_tree(final_clf_tree, filled=True,rounded=True,class_names=["No HD","Yes HD"], feature_names=X_train.columns)

"""The color code above is to show concentration of class. Orange shades - darker it is, more number of records with No HD"""

#confusion matrix with test data
ConfusionMatrixDisplay.from_estimator(final_clf_tree, X_test, y_test,display_labels=["No HD","Yes HD"])

print("Accuracy score", accuracy_score(y_test, final_clf_tree.predict(X_test)))
print("recall score", recall_score(y_test, final_clf_tree.predict(X_test)))
print("precision score", precision_score(y_test, final_clf_tree.predict(X_test)))

#there's little bit improvement with this test result compared to earlier.
#The point here is that this optimized tree will likely give decent results with different subsets
#earlier tree would have given less accuracy with different subset

"""Optimum alpha value can also be found by using - Grid search"""